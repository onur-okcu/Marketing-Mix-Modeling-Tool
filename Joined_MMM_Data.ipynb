{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b3be815-ed25-41c8-b80d-9002c8550131",
   "metadata": {},
   "source": [
    "## Processing User Data with the New Fields ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d113b91-84d4-4dea-9583-e98e2d9c537f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# install the packages\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, date, time, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6db544-512b-4225-9151-e5e4879bd4a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "import User_Data_Generation\n",
    "userdata = User_Data_Generation.users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c71846-bed4-489c-bc3e-48bbe2df8b17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Process data: convert fields into the right data types\n",
    "\n",
    "user_data = userdata.copy()\n",
    "user_data[\"timestamp\"] = pd.to_datetime(user_data[\"timestamp\"])\n",
    "\n",
    "# add new date column \n",
    "\n",
    "user_data[\"Date\"] = pd.to_datetime(user_data[\"timestamp\"]).dt.normalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5cab8c-4737-4dbc-8245-235b7982593d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "user_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414906ef-926c-41bb-973f-f2fa90e22726",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tackling \"Action\" field: extracting user actions to define attribution \n",
    "\n",
    "# Function to extract interactions\n",
    "def extract_interactions(history, position):\n",
    "    # Split the history string into a list\n",
    "    entries = history.split(\", \")\n",
    "    # Check if the requested position exists (from the end)\n",
    "    if len(entries) >= position:\n",
    "        return entries[-position]\n",
    "    return None  # Return None if the position doesn't exist\n",
    "\n",
    "# Apply the function to create the columns\n",
    "user_data[\"Last Interaction\"] = user_data[\"action\"].apply(lambda x: extract_interactions(x, 1))\n",
    "user_data[\"Third Interaction\"] = user_data[\"action\"].apply(lambda x: extract_interactions(x, 2))\n",
    "user_data[\"Second Interaction\"] = user_data[\"action\"].apply(lambda x: extract_interactions(x, 3))\n",
    "user_data[\"First Interaction\"] = user_data[\"action\"].apply(lambda x: extract_interactions(x, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef4067b-1e84-47de-929d-e408fa0ec3a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a new field \"Sales_Channel\" for attribution & further insights on the MMM\n",
    "\n",
    "# Define probabilities for Sales_Channel when Last Interaction is \"purchase\"\n",
    "sales_channel_probs = {\n",
    "    \"Search\": 0.4,\n",
    "    \"Social\": 0.25,\n",
    "    \"Video\": 0.15,\n",
    "    \"Display\": 0.10,\n",
    "    \"Digital Audio\": 0.10\n",
    "}\n",
    "\n",
    "# Function to determine Sales_Channel\n",
    "def determine_sales_channel(last_interaction):\n",
    "    if last_interaction in [\"view\", \"click\"]:\n",
    "        return \"Offline\"\n",
    "    else:\n",
    "        return np.random.choice(\n",
    "            list(sales_channel_probs.keys()), \n",
    "            p=list(sales_channel_probs.values())\n",
    "        )\n",
    "\n",
    "# Apply the function to create the Sales_Channel column\n",
    "user_data[\"Sales_Channel\"] = user_data[\"Last Interaction\"].apply(determine_sales_channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54dff49f-f39c-41de-aa29-627e6fac4cca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Testing a new function in a copied dataframe\n",
    "user_data2 = user_data.copy()\n",
    "\n",
    "# Getting view counts and click counts out of the action section\n",
    "\n",
    "click_probs = [0.35, 0.25, 0.15, 0.15, 0.1] # Search > Social > Video > Display > Audio\n",
    "view_probs = [0.1, 0.2, 0.25, 0.3, 0.15]  # Display > Social > Video > Search > Audio\n",
    "categories = [\"Search\", \"Social\", \"Video\", \"Display\", \"Audio\"]\n",
    "\n",
    "# Function to distribute actions\n",
    "def distribute_actions(actions):\n",
    "    purchases = actions.count(\"purchase\")\n",
    "    clicks = actions.count(\"click\") + purchases  # Purchases include 1 click each\n",
    "    views = actions.count(\"view\") + clicks  # Clicks (and purchases) include 1 view each\n",
    "    \n",
    "    # Distribute views and clicks based on probabilities\n",
    "    view_distribution = np.random.multinomial(views, view_probs)\n",
    "    click_distribution = np.random.multinomial(clicks, click_probs)\n",
    "    \n",
    "    # Create output dictionary\n",
    "    output = {f\"{category}_Views\": view_distribution[i] for i, category in enumerate(categories)}\n",
    "    output.update({f\"{category}_Clicks\": click_distribution[i] for i, category in enumerate(categories)})\n",
    "    return output\n",
    "\n",
    "# Apply the function and expand the result into new columns\n",
    "distributions = user_data2[\"action\"].apply(distribute_actions)\n",
    "distributions_user_data2 = pd.DataFrame(list(distributions))\n",
    "\n",
    "# Add the new columns to the original DataFrame\n",
    "user_data3 = pd.concat([user_data2, distributions_user_data2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c87b5aa-2b4f-41a2-ba71-4fd95b3205c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Feature Engineering\n",
    "\n",
    "user_data3[\"Sales_Channel\"].unique() # checking the number of unique values in Sales Channel\n",
    "user_data3[\"category\"].unique() # checking the number of unique values in Category\n",
    "user_data4 = user_data3.copy()\n",
    "\n",
    "# Import scikit-learn for OHE\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Initialize the OneHotEncoder\n",
    "encoder = OneHotEncoder(drop=None, sparse_output=False)\n",
    "\n",
    "# OneHotEncode \"category\"\n",
    "category_encoded = encoder.fit_transform(user_data4[[\"category\"]])\n",
    "category_col_encoded = encoder.get_feature_names_out([\"category\"])\n",
    "user_data4[category_col_encoded] = category_encoded  # Add encoded columns without removing the original\n",
    "\n",
    "# OneHotEncode \"Sales_Channel\"\n",
    "saleschannel_encoded = encoder.fit_transform(user_data4[[\"Sales_Channel\"]])\n",
    "sales_cha_encoded = encoder.get_feature_names_out([\"Sales_Channel\"])\n",
    "user_data4[sales_cha_encoded] = saleschannel_encoded  # Add encoded columns without removing the original\n",
    "\n",
    "# Display updated columns\n",
    "print(user_data4.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a311c2-9a58-48e1-805c-533d56ef4f5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ud = user_data4\n",
    "ud.head()\n",
    "ud.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c3a1e0-a8a1-4e2c-a85c-581d881c2941",
   "metadata": {},
   "source": [
    "## Joining User Data with the Macro & Spend Data ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887f58ab-ad1c-435b-88de-196fb86834d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# importing other notebook to join macro table with the user data\n",
    "\n",
    "import import_ipynb\n",
    "import MMM_macro_data \n",
    "daily = MMM_macro_data.daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f6dfd3-bc2c-415b-ae96-857f965e727f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Bringing the sum of revenue and sales on a daily rollup \n",
    "\n",
    "# Define the feature fields for summation\n",
    "feature_engineered_sum = [\n",
    "    'Search_Views', 'Social_Views', 'Video_Views', 'Display_Views', 'Audio_Views',\n",
    "    'Search_Clicks', 'Social_Clicks', 'Video_Clicks', 'Display_Clicks', 'Audio_Clicks',\n",
    "    'category_Books', 'category_Clothing', 'category_Electronics', 'category_Home',\n",
    "    'Sales_Channel_Digital Audio', 'Sales_Channel_Display', 'Sales_Channel_Offline',\n",
    "    'Sales_Channel_Search', 'Sales_Channel_Social', 'Sales_Channel_Video'\n",
    "]\n",
    "\n",
    "# Base aggregation dictionary\n",
    "agg_dictionary = {\n",
    "    \"revenue\": (\"price\", \"sum\"),\n",
    "    \"salescount\": (\"user_id\", \"count\"),\n",
    "    \"searchclicks\": (\"Search_Clicks\", \"sum\"),\n",
    "    \"search_impr\": (\"Search_Views\", \"sum\")\n",
    "}\n",
    "\n",
    "# Update the dictionary dynamically for all feature fields\n",
    "agg_dictionary.update({field: (field, \"sum\") for field in feature_engineered_sum})\n",
    "\n",
    "\n",
    "# Perform groupby operation\n",
    "ud_join = ud.groupby(\"Date\").agg(**agg_dictionary)\n",
    "\n",
    "# check the values\n",
    "\n",
    "ud_join.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff88819-606d-43c3-8833-f341f7d23922",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mmm_1 = daily.merge(ud_join, on = \"Date\", how=\"left\") # Joining the user data with the macro data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a7a38e-72e3-4384-bf72-4fa3c5072ff1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Macro data exploration\n",
    "\n",
    "mmm_1.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048a731b-f996-4a41-8305-7d8e980645bf",
   "metadata": {},
   "source": [
    "## MMM Joined Table EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cc2124-6ff4-43cb-aed9-73f3bc1ebc65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mmm_1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb073a2-f687-4a25-986b-1d474da170ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Checking correlation & distribution\n",
    "\n",
    "# Filter columns that contain 'Spend' in their names\n",
    "spendc = [col for col in mmm_1.columns if \"Spend\" in col] + [\"revenue\", \"salescount\"]\n",
    "\n",
    "# Create pairplot\n",
    "sns.pairplot(mmm_1[spendc], diag_kind=\"kde\", corner=True, height=2.5)\n",
    "\n",
    "# Show the plot\n",
    "plt.suptitle(\"Pairplot of Spend Fields\", y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecef3c27-7fbb-4fb1-85a5-69000fa923a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ensure correct syntax with explicit data parameter\n",
    "sns.lineplot(data=mmm_1, x=\"Date\", y=\"salescount\")\n",
    "plt.title(\"Sales Count Over Time\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Sales Count\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06dc2f67-0044-4cf1-90aa-47120e9a6d55",
   "metadata": {},
   "source": [
    "# Final Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bf8b11-5ef5-4e39-b7a6-03bbe9684795",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Drop columns containing \"Factor\" and specific columns 'searchclicks' and 'search_impr' to prevent data leakage\n",
    "\n",
    "mmm_2 = mmm_1.copy()\n",
    "\n",
    "mmm_2_columns_to_drop = [col for col in mmm_2.columns if \"Factor\" in col] + ['searchclicks', 'search_impr']\n",
    "mmm_2 = mmm_2.drop(columns=mmm_2_columns_to_drop)\n",
    "\n",
    "mmm_3 = mmm_2.copy()\n",
    "\n",
    "mmm_3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175ccde2-e659-4e60-a536-2807076970a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Monthly Data\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Convert Month, Year into date format and Weekday into string\n",
    "mmm_3[\"Month_Year\"] = pd.to_datetime(mmm_3[[\"Year\", \"Month\"]].assign(Day=1))\n",
    "mmm_3[\"Weekday\"] = mmm_3[\"Weekday\"].astype(str)\n",
    "\n",
    "# Group the table by Month_Year\n",
    "grouped_columns = {\n",
    "    \"Promo_Exists\": \"mean\",\n",
    "    \"Consumer_Index\": \"mean\",\n",
    "    \"Inflation_Rate\": \"mean\",\n",
    "}\n",
    "\n",
    "# Add all other numeric columns with sum\n",
    "for col in mmm_3.select_dtypes(include=\"number\").columns:\n",
    "    if col not in grouped_columns:\n",
    "        grouped_columns[col] = \"sum\"\n",
    "\n",
    "# Group and aggregate\n",
    "mmm_monthly = mmm_3.groupby(\"Month_Year\").agg(grouped_columns).reset_index()\n",
    "\n",
    "# Output the resulting monthly table\n",
    "mmm_monthly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0960824-0325-4ba4-a5ee-04b9df596f58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Turn Monthly Data into CSV to use it in different models such as LightweightMMM & PyMC \n",
    "\n",
    "mmm_monthly.to_csv(\"monthly_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90d5ae7-5b3c-42db-bc14-9da10cc914db",
   "metadata": {},
   "source": [
    "# Marketing Mix Modelling: Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c2c5c6-9ad9-40f7-a9ef-3e39db6f575f",
   "metadata": {},
   "source": [
    "#### Simple Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2eed9e-fdf0-4217-864c-dfa144c56894",
   "metadata": {},
   "source": [
    "Below is the simple baseline model based on the linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa3ce71-403a-458b-97e2-aa4c9efca092",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_percentage_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Prepare the data\n",
    "# Assuming `mmm_monthly` is the grouped monthly DataFrame from the previous step\n",
    "\n",
    "# Define predictors (X) and target variable (y)\n",
    "predictors = [\n",
    "    \"Promo_Exists\", \"Consumer_Index\", \"Inflation_Rate\", \"Search_Spend\",\n",
    "    \"Display_Spend\", \"Video_Spend\", \"Social_Spend\", \"Digital_Audio_Spend\",\n",
    "    \"TV_Spend\", \"Radio_Spend\", \"OOH_Spend\", \"Gross_Rating_Point\",\n",
    "    \"Sales_Channel_Digital Audio\", \"Sales_Channel_Display\",\n",
    "    \"Sales_Channel_Offline\", \"Sales_Channel_Search\", \"Sales_Channel_Social\",\n",
    "    \"Sales_Channel_Video\", \"Search_Views\", \"Social_Views\", \"Video_Views\",\n",
    "    \"Display_Views\", \"Audio_Views\", \"Search_Clicks\", \"Social_Clicks\",\n",
    "    \"Video_Clicks\", \"Display_Clicks\", \"Audio_Clicks\", \"category_Books\",\n",
    "    \"category_Clothing\", \"category_Electronics\", \"category_Home\"\n",
    "]\n",
    "\n",
    "target = \"revenue\"\n",
    "\n",
    "# Apply MinMaxScaler to selected columns\n",
    "scaler_columns = [col for col in predictors if \n",
    "                  \"_Views\" in col or \"_Clicks\" in col or \"category_\" in col or \"Sales_Channel_\" in col]\n",
    "scaler = MinMaxScaler()\n",
    "mmm_monthly[scaler_columns] = scaler.fit_transform(mmm_monthly[scaler_columns])\n",
    "\n",
    "X = mmm_monthly[predictors]\n",
    "y = mmm_monthly[target]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a simple linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Linear Regression Model Evaluation:\")\n",
    "print(f\"R-squared (R²): {r2:.2f}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
    "print(f\"Mean Absolute Percentage Error (MAPE): {mape:.2%}\")\n",
    "\n",
    "# Display the coefficients\n",
    "coefficients = pd.DataFrame({\n",
    "    \"Feature\": predictors,\n",
    "    \"Coefficient\": model.coef_\n",
    "}).sort_values(by=\"Coefficient\", ascending=False)\n",
    "\n",
    "print(\"\\nFeature Coefficients:\")\n",
    "print(coefficients)\n",
    "\n",
    "# Add predictions to the DataFrame for further analysis\n",
    "mmm_monthly[\"predicted_revenue\"] = model.predict(X)\n",
    "\n",
    "# Display a sample of the updated DataFrame\n",
    "print(\"\\nUpdated DataFrame with Predictions:\")\n",
    "print(mmm_monthly[[\"Month\", \"Year\", \"revenue\", \"predicted_revenue\"]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc839e6-1e26-4091-9a67-0b9b4b813293",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot: Actual vs. Predicted Revenue\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=y_test, y=y_pred, alpha=0.7)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', label=\"Perfect Fit\")\n",
    "plt.title(\"Actual vs. Predicted Revenue\")\n",
    "plt.xlabel(\"Actual Revenue\")\n",
    "plt.ylabel(\"Predicted Revenue\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d768c093-c6f9-40de-adf2-2c9f7819b306",
   "metadata": {},
   "source": [
    "### Baseline Model: Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5254c8-e73d-48d6-a7b7-3a307694af8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Lasso Model\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# Apply Lasso Regularization\n",
    "alpha = 100  # Adjust for optimal performance\n",
    "lasso_model = Lasso(alpha=alpha, random_state=42)\n",
    "lasso_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions and evaluation\n",
    "y_pred_lasso = lasso_model.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "mse_lasso = mean_squared_error(y_test, y_pred_lasso)\n",
    "r2_lasso = r2_score(y_test, y_pred_lasso)\n",
    "rmse_lasso = np.sqrt(mse_lasso)\n",
    "mape_lasso = mean_absolute_percentage_error(y_test, y_pred_lasso)\n",
    "\n",
    "\n",
    "print(\"Lasso Regression Evaluation:\")\n",
    "print(f\"Mean Squared Error (MSE): {mse_lasso:.2f}\")\n",
    "print(f\"R-squared (R²): {r2_lasso:.2f}\")\n",
    "print(f\"RMSE): {rmse_lasso:.2f}\")\n",
    "print(f\"MAPE: {mape_lasso:.2f}\")\n",
    "\n",
    "# Display coefficients\n",
    "coefficients = pd.DataFrame({\n",
    "    \"Feature\": predictors,\n",
    "    \"Coefficient\": lasso_model.coef_\n",
    "}).sort_values(by=\"Coefficient\", ascending=False)\n",
    "\n",
    "print(\"\\nFeature Coefficients (Lasso):\")\n",
    "print(coefficients)\n",
    "\n",
    "# Add predictions to the DataFrame\n",
    "mmm_monthly[\"predicted_revenue_lasso\"] = lasso_model.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a511f48-8a62-4840-bbb3-520e86019fd4",
   "metadata": {},
   "source": [
    "## XG Boost Based MMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c1f8a54-c416-44e4-8dff-77a73e017d99",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 23\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Initialize and train the XGBoost model\u001b[39;00m\n\u001b[1;32m     12\u001b[0m xgb_model \u001b[38;5;241m=\u001b[39m xgb\u001b[38;5;241m.\u001b[39mXGBRegressor(\n\u001b[1;32m     13\u001b[0m     objective\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreg:squarederror\u001b[39m\u001b[38;5;124m'\u001b[39m,  \u001b[38;5;66;03m# Regression objective\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m,             \u001b[38;5;66;03m# Number of boosting rounds\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     20\u001b[0m     early_stopping_rounds\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m\n\u001b[1;32m     21\u001b[0m )\n\u001b[0;32m---> 23\u001b[0m xgb_model\u001b[38;5;241m.\u001b[39mfit(\u001b[43mX_train\u001b[49m, y_train, eval_set\u001b[38;5;241m=\u001b[39m[(X_test, y_test)], verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Make predictions\u001b[39;00m\n\u001b[1;32m     26\u001b[0m y_pred_xgb \u001b[38;5;241m=\u001b[39m xgb_model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_percentage_error\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Initialize and train the XGBoost model\n",
    "xgb_model = xgb.XGBRegressor(\n",
    "    objective='reg:squarederror',  # Regression objective\n",
    "    n_estimators=1000,             # Number of boosting rounds\n",
    "    learning_rate=0.05,             # Learning rate\n",
    "    max_depth=5,                    # Maximum tree depth\n",
    "    subsample=0.8,                   # Subsample ratio\n",
    "    colsample_bytree=0.8,             # Column sampling\n",
    "    random_state=42,\n",
    "    early_stopping_rounds=10\n",
    ")\n",
    "\n",
    "xgb_model.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=False)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "r2_xgb = r2_score(y_test, y_pred_xgb)\n",
    "mse_xgb = mean_squared_error(y_test, y_pred_xgb)\n",
    "rmse_xgb = np.sqrt(mse_xgb)\n",
    "mape_xgb = mean_absolute_percentage_error(y_test, y_pred_xgb)\n",
    "\n",
    "print(\"XGBoost Model Evaluation:\")\n",
    "print(f\"R-squared (R²): {r2_xgb:.2f}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse_xgb:.2f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse_xgb:.2f}\")\n",
    "print(f\"Mean Absolute Percentage Error (MAPE): {mape_xgb:.2%}\")\n",
    "\n",
    "# Feature importance plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "xgb.plot_importance(xgb_model, max_num_features=10, importance_type=\"weight\")\n",
    "plt.title(\"Top 10 Feature Importance\")\n",
    "plt.show()\n",
    "\n",
    "# Add predictions to the original DataFrame\n",
    "mmm_monthly[\"predicted_revenue_xgb\"] = xgb_model.predict(X)\n",
    "\n",
    "# Plot actual vs predicted revenue\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(x=mmm_monthly['Month_Year'], y=mmm_monthly['revenue'], label='Actual Revenue', color='blue')\n",
    "sns.lineplot(x=mmm_monthly['Month_Year'], y=mmm_monthly['predicted_revenue_xgb'], label='Predicted Revenue', color='red')\n",
    "plt.title('Actual vs Predicted Revenue Over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Revenue')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858ac414-2c9a-45d2-96a4-0e6b75a5c88f",
   "metadata": {},
   "source": [
    "# Random Forest Algorithm with Grid Search "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1171edd-9638-4cc8-b2ca-bf48ce204534",
   "metadata": {},
   "source": [
    "In addition to Linear Regression, Lasso model and the XGBoost which are instrumental to solve regression problems, Random Forest algorithm by picking the best estimator thanks to the Grid Search could be very instrumental to understand which marketing channel would perform the best in the context of an MMM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecc1157-04ad-4f98-9942-5862bc7b5c52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
